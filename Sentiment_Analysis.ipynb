{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8e206d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8b9c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e0d317",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"D:\\\\deep learning\\\\senti\\\\sentiment.csv\")\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b77f5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['text','sentiment']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9be06a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06487e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.sentiment != \"Neutral\"]\n",
    "data['sentiment']= pd.get_dummies(data['sentiment'], drop_first = True)\n",
    "data = data.reset_index(drop=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bc0202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ae454d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "\n",
    "for i in range(0, data.shape[0]):\n",
    "    tweet = re.sub('[^a-zA-Z]', ' ', data['text'][i])\n",
    "    tweet = tweet.lower()\n",
    "    tweet = word_tokenize(tweet)\n",
    "    \n",
    "    # Reduce words to their root form\n",
    "    tweet = [WordNetLemmatizer().lemmatize(w) for w in tweet if not w in set(stopwords.words('english'))]\n",
    "    \n",
    "    # Lemmatize verbs by specifying pos\n",
    "    tweet = [WordNetLemmatizer().lemmatize(w, pos='v') for w in tweet if not w in set(stopwords.words('english'))]\n",
    "    tweet = ' '.join(tweet)\n",
    "    corpus.append(tweet)\n",
    "\n",
    "print(corpus[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9078d360",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfIdfVectorizer=TfidfVectorizer(use_idf=True)\n",
    "tfIdf = tfIdfVectorizer.fit_transform(corpus)\n",
    "df = pd.DataFrame(tfIdf[0].T.todense(), index=tfIdfVectorizer.get_feature_names_out(), columns=[\"TF-IDF\"])\n",
    "df = df.sort_values('TF-IDF', ascending=False)\n",
    "pd.set_option('display.max_rows', None)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa05b9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(corpus)):\n",
    "    corpus[i] = re.sub('co','', corpus[i])\n",
    "    corpus[i] = re.sub('rt','', corpus[i])\n",
    "    corpus[i] = re.sub('http','', corpus[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0e1f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=None)\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "encoded_docs = tokenizer.texts_to_sequences(corpus)\n",
    "padded_sequence = pad_sequences(encoded_docs,maxlen=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aff4f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_sequence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ebc168",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.word_index['trump'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2db7f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus[0])\n",
    "print(encoded_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf4147a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(padded_sequence[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2810d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM,Dense, Dropout\n",
    "from tensorflow.keras.layers import SpatialDropout1D\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from keras.initializers import Constant\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_vector_length = 200\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_vector_length,\n",
    "                    input_length=25) )\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(3, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6466ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the targets to numpy array to feed it into the model\n",
    "target = np.asarray(data['sentiment'])\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cdc22b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MODEL = model.fit(padded_sequence,target,validation_split=0.2, epochs=5, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349fa421",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_word ='Pune is a not a good city'\n",
    "tw = tokenizer.texts_to_sequences([test_word])\n",
    "tw = pad_sequences(tw,maxlen=25)\n",
    "sentiment = int(model.predict(tw).round().item())\n",
    "#print(sentiment)\n",
    "if sentiment==0:\n",
    "    print(\"Negative\")\n",
    "else:\n",
    "    print(\"Positive\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fe57f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c1696a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
